{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "\n",
    "#### Due Date: 24th Jan'18\n",
    "\n",
    "In this assignment we will cover the basics of Machine Learning. We will cover the following topics:\n",
    "\n",
    "1) Linear Regression\n",
    "\n",
    "2) Logistic Regression\n",
    "\n",
    "3) EM Algorithm\n",
    "\n",
    "4) K-means/Hirarchical Clustering.\n",
    "\n",
    "It is crucial to get down to the nitty gritty of the code to implement all of these. No external packages (like scipy), which directly give functions for these algorithms, are to be used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "Defination: Given a data set ${\\displaystyle \\{y_{i},\\,x_{i1},\\ldots ,x_{ip}\\}_{i=1}^{n}} $ of $n$ statistical units, a linear regression model assumes that the relationship between the dependent variable $y_i$ and the $p$-vector of regressors $x_i$ is linear. This relationship is modeled through a disturbance term or error variable $Îµ_i$ - an unobserved random variable that adds noise to the linear relationship between the dependent variable and regressors. Thus the model takes the form:\n",
    "\n",
    "$$ {\\displaystyle \\mathbf {y} =X{\\boldsymbol {\\beta }}+{\\boldsymbol {\\varepsilon }},\\,} $$\n",
    "\n",
    "where,\n",
    "\n",
    "$$ \\mathbf {y} ={\\begin{pmatrix}y_{1}\\\\y_{2}\\\\\\vdots \\\\y_{n}\\end{pmatrix}},\\quad $$\n",
    "\n",
    "$$ {\\displaystyle X={\\begin{pmatrix}\\mathbf {x} _{1}^{\\top }\\\\\\mathbf {x} _{2}^{\\top }\\\\\\vdots \\\\\\mathbf {x} _{n}^{\\top }\\end{pmatrix}}={\\begin{pmatrix}1&x_{11}&\\cdots &x_{1p}\\\\1&x_{21}&\\cdots &x_{2p}\\\\\\vdots &\\vdots &\\ddots &\\vdots \\\\1&x_{n1}&\\cdots &x_{np}\\end{pmatrix}},}\n",
    "$$\n",
    "\n",
    "$$ {\\displaystyle {\\boldsymbol {\\beta }}={\\begin{pmatrix}\\beta _{0}\\\\\\beta _{1}\\\\\\beta _{2}\\\\\\vdots \\\\\\beta _{p}\\end{pmatrix}},\\quad {\\boldsymbol {\\varepsilon }}={\\begin{pmatrix}\\varepsilon _{1}\\\\\\varepsilon _{2}\\\\\\vdots \\\\\\varepsilon _{n}\\end{pmatrix}}.} \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this problem, in the class lecture we covered the Least Square Solution, which can be formulated as:\n",
    "\n",
    "$${\\displaystyle {\\hat {\\boldsymbol {\\beta }}}=(\\mathbf {X} ^{\\top }\\mathbf {X} )^{-1}\\mathbf {X} ^{\\top }\\mathbf {y} =\\left(\\sum \\mathbf {x} _{i}\\mathbf {x} _{i}^{\\top }\\right)^{-1}\\left(\\sum \\mathbf {x} _{i}y_{i}\\right).} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "a) You will write the code to find the LSS for the given data. The data contains 3 columns, each for $y, X_{1}$, and $X_{2}$ respectively. Few of the possible models are:\n",
    "\n",
    "$$ {\\displaystyle \\mathbf {y} ={\\boldsymbol {\\beta_{1} }}X_1+{\\boldsymbol {\\beta_{0} }}+{\\boldsymbol {\\varepsilon }},\\,} $$\n",
    "\n",
    "\n",
    "$$ {\\displaystyle \\mathbf {y} ={\\boldsymbol {\\beta_{2} }}X_2+{\\boldsymbol {\\beta_{0} }}+{\\boldsymbol {\\varepsilon }},\\,} $$\n",
    "\n",
    "\n",
    "$$ {\\displaystyle \\mathbf {y} ={\\boldsymbol {\\beta_{1} }}X_1+ {\\boldsymbol {\\beta_{2} }}X_2+{\\boldsymbol {\\beta_{0} }}+{\\boldsymbol {\\varepsilon }},\\,} $$\n",
    "\n",
    "Given this data, find the coefficients for each of these models.\n",
    "\n",
    "b) Now that you have three models, you must select the best one. Use Cross-validation with 5 folds on the dataset to find the optimal model (On the basis of RMSE on the test partition). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Load the dataset \n",
    "# train_data = np.load('utils/assign_1_data_1_train.npy')\n",
    "# now write the code for finding the solution for each of the three models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE for Model-1 using only X_1 is [[ 1993192.99407075]]\n",
      "The regression weights for Model-1 is [[-208.39095947]\n",
      " [  82.55935339]]\n",
      "The RMSE for Model-2 using only X_2 is [[ 4987680.33069842]]\n",
      "The regression weights for Model-2 is [[    4.85238331]\n",
      " [-2694.47627382]]\n",
      "The RMSE for Model-3 using both X_1 and X_2 is [[ 1945408.68433806]]\n",
      "The regression weights for Model-3 is [[-206.47337776]\n",
      " [   3.04933431]\n",
      " [-947.95740796]]\n"
     ]
    }
   ],
   "source": [
    "# Finally, Write The estimates of the betas here:\n",
    "def normal_equation(X,y):\n",
    "    X_t=np.transpose(X)\n",
    "    beta=np.dot(np.dot(np.linalg.inv(np.dot(X_t,X)),X_t),y)\n",
    "    return beta\n",
    "\n",
    "def rmse(X,y,beta):\n",
    "    y_pred=np.dot(X,beta)\n",
    "    error=y_pred-y\n",
    "    error_t=np.transpose(error)\n",
    "    RMSE=np.dot(error_t,error)/(y.shape[0])\n",
    "    return RMSE\n",
    "\n",
    "train_data = np.load('utils/assign_1_data_1_train.npy')\n",
    "# Model 1\n",
    "\n",
    "\n",
    "\n",
    "X_1 = train_data[:,1]\n",
    "X_2 = train_data[:,2]\n",
    "bias_1=np.ones(X_1.shape)\n",
    "y=train_data[:,0]\n",
    "\n",
    "X_1=np.expand_dims(X_1,1)\n",
    "bias_1=np.expand_dims(bias_1,1)\n",
    "y=np.expand_dims(y,1)\n",
    "\n",
    "X_1 = np.hstack((X_1,bias_1))\n",
    "\n",
    "\n",
    "beta_1 = normal_equation(X_1,y)\n",
    "RMSE=rmse(X_1,y,beta_1)\n",
    "print(u'The RMSE for Model-1 using only X_1 is ' + str(RMSE))\n",
    "print(u'The regression weights for Model-1 is ' + str(beta_1))\n",
    "\n",
    "# Model 2\n",
    "\n",
    "\n",
    "bias_2=np.ones(X_2.shape)\n",
    "X_2=np.expand_dims(X_2,1)\n",
    "bias_2=np.expand_dims(bias_2,1)\n",
    "X_2 = np.hstack((X_2,bias_2))\n",
    "beta_2 = normal_equation(X_2,y)\n",
    "RMSE=rmse(X_2,y,beta_2)\n",
    "\n",
    "print(u'The RMSE for Model-2 using only X_2 is ' + str(RMSE))\n",
    "print(u'The regression weights for Model-2 is ' + str(beta_2))\n",
    "\n",
    "# Model 3\n",
    "X=train_data[:,1:]\n",
    "\n",
    "# a=X.min(axis=0)\n",
    "# b=X.ptp(axis=0)\n",
    "# X= (X - a) /b\n",
    "\n",
    "bias_3 = np.ones((X.shape[0],1))\n",
    "X = np.hstack((X,bias_3))\n",
    "beta_3 = normal_equation(X,y)\n",
    "RMSE=rmse(X,y,beta_3)\n",
    "\n",
    "print(u'The RMSE for Model-3 using both X_1 and X_2 is ' + str(RMSE))\n",
    "print(u'The regression weights for Model-3 is ' + str(beta_3))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partition the dataset into 5 random folds.\n",
    "# X_train=X\n",
    "# Y_train=y\n",
    "\n",
    "# folds=\n",
    "\n",
    "\n",
    "    \n",
    "# for each fold, approx. model from the remaining folds, and calculate RMSE on the test fold.\n",
    "\n",
    "# find avg RMSE for each model. \n",
    "\n",
    "# Which is the best model?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, Give the R^2 score of the best model in the test set:\n",
    "test_data = np.load('utils/assign_1_data_1_test.npy')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus\n",
    "\n",
    "# Show a graph between the predicted Y^ and the Ground truth Y\n",
    "\n",
    "# Try to plot Y vs X_1 in train set.\n",
    "\n",
    "# can it help you improve your model?\n",
    "# construct the better model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Generaly, Logistic Regression is used to predict categorial variables. For the simple problem of 2-way classification, the output $\\hat{y_i}$ is modeled as the probability that $\\{X_i\\}$ belongs to class $1$ (given two classes $0$, and $1$).\n",
    "\n",
    "$$ P( \\{X_i\\} \\in Set_1 ) = \\hat{y_i}, $$ ( $y_i$ is the actual label; $y_i \\in \\{ 0,1 \\}$ )\n",
    "\n",
    "\n",
    "$\\hat{y_i}$ is typically modeled as the output of a sigmoid on a linear combination of the input feature $\\{X_i\\}$:\n",
    "\n",
    "$$ \\mathbf {\\hat{y}} = \\sigma(X{\\boldsymbol {\\beta }}+{\\boldsymbol {\\varepsilon }}) = \\sigma_\\beta(X)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, The likelihood of some given data for this model can be written as:\n",
    "\n",
    "$${\\displaystyle {\\begin{aligned}L(\\beta |x)&=Pr(Y|X;\\beta )\\\\&=\\prod _{i}Pr(y_{i}|x_{i};\\beta )\\\\&=\\prod _{i}\\sigma_{\\beta }(x_{i})^{y_{i}}(1-\\sigma_{\\beta }(x_{i}))^{(1-y_{i})}\\end{aligned}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike in the case of Linear regression, this equation has no closed form solution. Hence we will use gradient descent on the negative log-likelihood $J(\\beta)$ to find the optimal $\\beta$\n",
    "\n",
    "$$\n",
    "J(\\beta) = \\sum_i{\\big( y_ilog(\\hat{y_i})+ (1-y_i)log(1-\\hat{y_i}) \\big) }\n",
    "$$\n",
    "\n",
    "with the update equation:\n",
    "\n",
    "$$\n",
    "\\beta_j = \\beta_j + \\alpha \\times \\frac{ \\partial J(\\beta)}{\\partial \\beta}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "a) You will write the code to find the optimal logistic regression model for the given data. The data contains 3 columns, each for $y, X_{1}$, and $X_{2}$ respectively. For the rate of learning $\\alpha$ use a linearly decaying policy, or step-wise reduction policy. \n",
    "\n",
    "$$ {\\displaystyle \\mathbf {y} =\\sigma \\big( {\\boldsymbol {\\beta_{1} }}X_1+ {\\boldsymbol {\\beta_{2} }}X_2+{\\boldsymbol {\\beta_{0} }}+{\\boldsymbol {\\varepsilon }}} \\big) $$\n",
    "\n",
    "b) Explore possible methods of adjusting the learning rate $\\alpha$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number0 train error = 0.603568479981 validation error = 0.544297700941\n",
      "Iteration number10 train error = 0.476313578282 validation error = 0.435413155683\n",
      "Iteration number20 train error = 0.420628770566 validation error = 0.387484030863\n",
      "Iteration number30 train error = 0.391379882489 validation error = 0.3630959659\n",
      "Iteration number40 train error = 0.374060848307 validation error = 0.349175603389\n",
      "Iteration number50 train error = 0.362928712136 validation error = 0.340587651772\n",
      "Iteration number60 train error = 0.355348404442 validation error = 0.335002512893\n",
      "Iteration number70 train error = 0.349964477661 validation error = 0.331235791821\n",
      "Iteration number80 train error = 0.346016634464 validation error = 0.328631135376\n",
      "Iteration number90 train error = 0.34304901657 validation error = 0.32679998972\n",
      "Iteration number100 train error = 0.340773590465 validation error = 0.325500148379\n",
      "Iteration number110 train error = 0.339000538543 validation error = 0.324574289207\n",
      "Iteration number120 train error = 0.337600388837 validation error = 0.323916763041\n",
      "Iteration number130 train error = 0.33648227029 validation error = 0.323454655267\n",
      "Iteration number140 train error = 0.335580852043 validation error = 0.323136490564\n"
     ]
    }
   ],
   "source": [
    "# Load the train dataset \n",
    "train_data = np.load('utils/assign_1_data_2_train.npy')\n",
    "# now write the code to find the parameters of the optimization.\n",
    "\n",
    "\n",
    "X_train=train_data[:,1:]\n",
    "Y_train=train_data[:,0]\n",
    "\n",
    "\n",
    "X_train_new=train_data[30:,1:]\n",
    "Y_train_new=train_data[30:,0]\n",
    "\n",
    "X_val=train_data[:30,1:]\n",
    "Y_val=train_data[:30,0]\n",
    "\n",
    "Y_train = np.expand_dims(Y_train,1)\n",
    "Y_train_new = np.expand_dims(Y_train_new,1)\n",
    "Y_val = np.expand_dims(Y_val,1)\n",
    "\n",
    "# a=X_train.min(axis=0)\n",
    "# b=X_train.ptp(axis=0)\n",
    "# x_normed = (X_train - a) /b\n",
    "\n",
    "\n",
    "a=X_train_new.min(axis=0)\n",
    "b=X_train_new.ptp(axis=0)\n",
    "x_normed = (X_train_new - a) /b\n",
    "\n",
    "\n",
    "\n",
    "X_val_normed= (X_val-a)/b\n",
    "\n",
    "weights=np.ones((2,1))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def cost(y, t):\n",
    "    return - np.sum(np.multiply(t, np.log(y)) + np.multiply((1-t), np.log(1-y)))\n",
    "\n",
    "def logit(x,weights):\n",
    "    return sigmoid(np.dot(x,weights))\n",
    "\n",
    "alpha=0.01\n",
    "nb_epochs=150\n",
    "for i in range(nb_epochs):\n",
    "    y=logit(x_normed,weights)\n",
    "    gradient= Y_train_new - y\n",
    "    x_normed_t=np.transpose(x_normed)\n",
    "    update= np.dot(x_normed_t,gradient)\n",
    "    weights=weights + (alpha*update)\n",
    "    y_val=logit(X_val_normed,weights)\n",
    "    \n",
    "    if i%10==0:\n",
    "        alpha=alpha\n",
    "        print(u'Iteration number'+ str(i)+ u' train error = ' + str(cost(y,Y_train_new)/270)+ u' validation error = ' + str(cost(y_val,Y_val)/30)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number0 train error = 0.59940156801\n",
      "Iteration number10 train error = 0.464464142603\n",
      "Iteration number20 train error = 0.409769063083\n",
      "Iteration number30 train error = 0.382341158743\n",
      "Iteration number40 train error = 0.366571680425\n",
      "Iteration number50 train error = 0.356655282108\n",
      "Iteration number60 train error = 0.350024923562\n",
      "Iteration number70 train error = 0.345391904488\n",
      "Iteration number80 train error = 0.342046057566\n",
      "Iteration number90 train error = 0.33956749306\n",
      "Iteration number100 train error = 0.337694011157\n",
      "Iteration number110 train error = 0.336254615153\n",
      "Iteration number120 train error = 0.335133790018\n",
      "Iteration number130 train error = 0.334251200813\n",
      "Iteration number140 train error = 0.333549604544\n"
     ]
    }
   ],
   "source": [
    "# test on a validation part every 't' iterations to find when you start overfitting.\n",
    "# t = ?\n",
    "\n",
    "# Now for 't' iterations train on the entire dataset for testing on the test_data\n",
    "\n",
    "a=X_train.min(axis=0)\n",
    "b=X_train.ptp(axis=0)\n",
    "x_normed = (X_train - a) /b\n",
    "\n",
    "t=150\n",
    "\n",
    "alpha=0.01\n",
    "nb_epochs=t\n",
    "weights=np.ones((2,1))\n",
    "\n",
    "for i in range(nb_epochs):\n",
    "    y=logit(x_normed,weights)\n",
    "    gradient= Y_train - y\n",
    "    x_normed_t=np.transpose(x_normed)\n",
    "    update= np.dot(x_normed_t,gradient)\n",
    "    weights=weights + (alpha*update)\n",
    "    \n",
    "    if i%10==0:\n",
    "        alpha=alpha\n",
    "        print(u'Iteration number'+ str(i)+ u' train error = ' + str(cost(y,Y_train)/300)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test dataset is 51.7333333333 percent\n"
     ]
    }
   ],
   "source": [
    "# find the accuracy on the test set:\n",
    "test_data = np.load('utils/assign_1_data_2_test.npy')\n",
    "\n",
    "X_test = test_data[:,1:]\n",
    "Y_test = test_data[:,0]\n",
    "\n",
    "predictions= logit(X_test,weights)\n",
    "\n",
    "\n",
    "tolerance=0.5\n",
    "accuracy =(np.abs(predictions - Y_test) < tolerance ).mean()\n",
    "\n",
    "print(u'Accuracy on test dataset is '  +  str(accuracy*100) +u' percent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus\n",
    "# Can you adjust the learning rate alpha in a better way?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EM algorithm\n",
    "\n",
    "This is a general framework for likelihood-based parameter estimation.\n",
    "A basic outline of this algorithm is:\n",
    "\n",
    "* start with initial guesses of parameters\n",
    "\n",
    "* E step: estimate memberships given params\n",
    "\n",
    "* M step: estimate params given memberships\n",
    "\n",
    "* Repeat until convergence\n",
    "\n",
    "** Refer to [this link](http://www.rmki.kfki.hu/~banmi/elte/bishop_em.pdf) (9.2.2) .**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "Let ${\\displaystyle \\mathbf {x} =(\\mathbf {x} _{1},\\mathbf {x} _{2},\\ldots ,\\mathbf {x} _{n})} $ be a sample of $n$ independent observations from a mixture of two multivariate normal distributions of dimension $d$ , and let ${\\displaystyle \\mathbf {z} =(z_{1},z_{2},\\ldots ,z_{n})} $ be the latent variables that determine the component from which the observation originates.\n",
    "\n",
    "$X_i |(Z_i = 1) \\sim \\mathcal{N}_d(\\boldsymbol{\\mu}_1,\\Sigma_1)$ and $X_i |(Z_i = 2) \\sim \\mathcal{N}_d(\\boldsymbol{\\mu}_2,\\Sigma_2)$\n",
    "\n",
    "The aim is to estimate the unknown parameters representing the mixing value between the Gaussians and the means and covariances of each:\n",
    "\n",
    "$$ \\theta = \\big( \\boldsymbol{\\tau},\\boldsymbol{\\mu}_1,\\boldsymbol{\\mu}_2,\\Sigma_1,\\Sigma_2 \\big) $$\n",
    "\n",
    "a) Given the data, find the parameters $\\theta$ using EM algorithm.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train dataset \n",
    "data = np.load('utils/assign_1_data_3.npy')\n",
    "# The data is a 1000*2 numpy array, where each row is a independent observation, and \n",
    "# the columns are measurement in dimension x and y respectively. \n",
    "# now write the code to find the parameter theta.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters are given by:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the entire data by plotting them as points in a 2-D canvas.  \n",
    "# Show the estimated means and the standard deviations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "For clustering we covered two algorithms\n",
    "\n",
    "1) K-means : An iterative method to get 'K' clusters, initializing them randomly\n",
    "\n",
    "2) Hirarchical : An iterative method to get a dendogram of clustering with various numbers of cluster centers.\n",
    "\n",
    "### K-means Clustering\n",
    "\n",
    "We initialize $K$ cluster centers $\\{ c_1,c_2 ,... c_k\\}$for $K$-clusters randomly. All the data points are assigned a cluster index $D_i \\in \\{ 1,2,...,k\\}$, based on the closest cluster center to each point.\n",
    "\n",
    "Now, for each cluster, the cluster centers are re-evaluated as the mean of all the points in the center.\n",
    "\n",
    "$$\n",
    "c_i = mean(\\{ X_j | D_j = i \\})\n",
    "$$\n",
    "This process continues till convergence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "The dataset contains 1000  color images.Convert them to grayscale images. We need to cluster them into various $n$ clusters based on the similarity of their histograms. For each image, find the histogram with bin size 25 (last bin of 30;i.e.225-255;giving you 10 bins). Now treating each of these bins as seperate dimensions, find:\n",
    "\n",
    "a) Cluster Centers for $n = 5$ clusters, with $L_2$ distance criteria for measuring distance between a pair of images.\n",
    "\n",
    "b) **Bonus**: Use Earth Movers Distance in the above problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this problem we will be using the 1000 test images of CIFAR-10 dataset.\n",
    "## Load the data from the following link\n",
    "# https://www.cs.toronto.edu/~kriz/cifar.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert it to grayscale values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the histograms and get a 10-dimensional representation of each images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use K-means to find  out the number of cluster centers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cluster means to see what they look like.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "Useful references will be added shortly.\n",
    "\n",
    "1) Linear Regression\n",
    "  * [Wikipedia](https://en.wikipedia.org/wiki/Linear_regression)\n",
    "\n",
    "2) Logistic Regression\n",
    "  * [Wikipedia](https://en.wikipedia.org/wiki/Logistic_regression)\n",
    "  * [Win Vector Blog](http://www.win-vector.com/blog/2011/09/the-simpler-derivation-of-logistic-regression/)\n",
    "  * [Renselaer Course Slides](http://www.cs.rpi.edu/~magdon/courses/LFD-Slides/SlidesLect09.pdf)\n",
    "  \n",
    "3) EM\n",
    "  * [Cambridge Tutorial](http://www.cs.huji.ac.il/~yweiss/emTutorial.pdf)\n",
    "  * [Chapter 9 - Pattern Recognition and Machine Learning by Christopher M. Bishop](http://www.rmki.kfki.hu/~banmi/elte/bishop_em.pdf)\n",
    "  * [Wikipedia](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm)\n",
    "  \n",
    "4) K-means\n",
    "  * [Wikipedia](https://en.wikipedia.org/wiki/K-means_clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
